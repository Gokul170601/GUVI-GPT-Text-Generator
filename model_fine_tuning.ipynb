{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":91876,"status":"ok","timestamp":1719470198217,"user":{"displayName":"GOKUL M","userId":"10869017017326738276"},"user_tz":-330},"id":"F4dGMLQ-8Ekh","outputId":"8f0d9124-5425-43a7-d008-3775c9246530"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n","Collecting datasets\n","  Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.3)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n","Collecting pyarrow>=15.0.0 (from datasets)\n","  Downloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n","Collecting requests (from transformers)\n","  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting xxhash (from datasets)\n","  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n","  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n","  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Installing collected packages: xxhash, requests, pyarrow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, datasets\n","  Attempting uninstall: requests\n","    Found existing installation: requests 2.31.0\n","    Uninstalling requests-2.31.0:\n","      Successfully uninstalled requests-2.31.0\n","  Attempting uninstall: pyarrow\n","    Found existing installation: pyarrow 14.0.2\n","    Uninstalling pyarrow-14.0.2:\n","      Successfully uninstalled pyarrow-14.0.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n","google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n","ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed datasets-2.20.0 dill-0.3.8 multiprocess-0.70.16 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 pyarrow-16.1.0 requests-2.32.3 xxhash-3.4.1\n"]}],"source":["pip install transformers datasets torch"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16160,"status":"ok","timestamp":1719470223823,"user":{"displayName":"GOKUL M","userId":"10869017017326738276"},"user_tz":-330},"id":"mIkcdpNQ8Bd4","outputId":"e083390c-e30b-4a17-c89b-0a8b359179db"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting accelerate\n","  Downloading accelerate-0.31.0-py3-none-any.whl (309 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m309.4/309.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.0+cu121)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.4)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.15.3)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.20.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.5.40)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.6.2)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","Installing collected packages: accelerate\n","Successfully installed accelerate-0.31.0\n"]}],"source":["pip install accelerate -U"]},{"cell_type":"markdown","source":["# Data Preprocessing"],"metadata":{"id":"HaZRuYypJB6u"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"626jUWbO6Vx7"},"outputs":[],"source":["import os\n","import re\n","from transformers import GPT2Tokenizer\n","\n","def preprocess_data(input_file, output_file, tokenizer_name=\"gpt2\"):\n","    tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_name)\n","    with open(input_file, 'r', encoding='utf-8') as f:\n","        lines = f.readlines()\n","\n","    with open(output_file, 'w', encoding='utf-8') as f:\n","        for line in lines:\n","            line = line.lower()\n","\n","            # Tokenize the line\n","            token_ids = tokenizer.encode(line, add_special_tokens=False)\n","            # Convert token IDs back to tokens\n","            tokenized_line = tokenizer.convert_ids_to_tokens(token_ids)\n","            # Convert tokens to text and remove special tokens\n","            processed_line = \" \".join(tokenized_line).replace('Ä ', '').replace('ÄŠ', '').replace('ï¿½', '').strip()\n","\n","            processed_line = re.sub(r'[^a-zA-Z0-9\\s,():%&]', '', processed_line)\n","            processed_line = re.sub(r'\\bgu vi\\b', 'guvi', processed_line)\n","\n","            # Remove extra spaces\n","            processed_line = re.sub(r'\\s+', ' ', processed_line)\n","            # Write the processed line to the output file\n","            f.write(processed_line + \"\\n\")\n","\n","input_file = \"/content/dummy.txt\"  # Make sure this path is correct\n","output_file = \"processed_data.txt\"\n","preprocess_data(input_file, output_file)"]},{"cell_type":"markdown","source":["# Fine Tuning Model"],"metadata":{"id":"s_tBDQPfJKDT"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":945},"executionInfo":{"elapsed":2341016,"status":"ok","timestamp":1719472773504,"user":{"displayName":"GOKUL M","userId":"10869017017326738276"},"user_tz":-330},"id":"XXRhl40rplOf","outputId":"478810b4-f62c-4cc1-c4fd-51b5b42aaf9d"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ğŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='11615' max='11615' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [11615/11615 38:54, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>2.777700</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>1.627700</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>1.101700</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.754800</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.563100</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.417500</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>0.350400</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>0.300200</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>0.260900</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>0.220400</td>\n","    </tr>\n","    <tr>\n","      <td>5500</td>\n","      <td>0.198400</td>\n","    </tr>\n","    <tr>\n","      <td>6000</td>\n","      <td>0.187700</td>\n","    </tr>\n","    <tr>\n","      <td>6500</td>\n","      <td>0.174500</td>\n","    </tr>\n","    <tr>\n","      <td>7000</td>\n","      <td>0.168500</td>\n","    </tr>\n","    <tr>\n","      <td>7500</td>\n","      <td>0.147600</td>\n","    </tr>\n","    <tr>\n","      <td>8000</td>\n","      <td>0.142700</td>\n","    </tr>\n","    <tr>\n","      <td>8500</td>\n","      <td>0.139800</td>\n","    </tr>\n","    <tr>\n","      <td>9000</td>\n","      <td>0.136700</td>\n","    </tr>\n","    <tr>\n","      <td>9500</td>\n","      <td>0.129900</td>\n","    </tr>\n","    <tr>\n","      <td>10000</td>\n","      <td>0.123500</td>\n","    </tr>\n","    <tr>\n","      <td>10500</td>\n","      <td>0.120700</td>\n","    </tr>\n","    <tr>\n","      <td>11000</td>\n","      <td>0.121700</td>\n","    </tr>\n","    <tr>\n","      <td>11500</td>\n","      <td>0.117600</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["('./fine_tuned_model/tokenizer_config.json',\n"," './fine_tuned_model/special_tokens_map.json',\n"," './fine_tuned_model/vocab.json',\n"," './fine_tuned_model/merges.txt',\n"," './fine_tuned_model/added_tokens.json')"]},"metadata":{},"execution_count":6}],"source":["from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, TextDataset, DataCollatorForLanguageModeling\n","\n","# Load pre-trained model and tokenizer\n","model_name = \"gpt2\"\n","tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n","model = GPT2LMHeadModel.from_pretrained(model_name)\n","\n","# Create dataset\n","def load_dataset(file_path, tokenizer, block_size=128):\n","    return TextDataset(\n","        tokenizer=tokenizer,\n","        file_path=file_path,\n","        block_size=block_size,\n","    )\n","\n","train_dataset = load_dataset(output_file, tokenizer)\n","\n","# Define training arguments\n","training_args = TrainingArguments(\n","    output_dir=\"./results\",\n","    overwrite_output_dir=True,\n","    num_train_epochs=5,\n","    per_device_train_batch_size=4,\n","    save_steps=10_000,\n","    save_total_limit=2,\n",")\n","\n","# Initialize data collator\n","data_collator = DataCollatorForLanguageModeling(\n","    tokenizer=tokenizer,\n","    mlm=False,\n",")\n","\n","# Initialize Trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=train_dataset,\n",")\n","\n","# Fine-tune the model\n","trainer.train()\n","\n","# Save the fine-tuned model and tokenizer\n","model.save_pretrained(\"./fine_tuned_model\")\n","tokenizer.save_pretrained(\"./fine_tuned_model\")"]},{"cell_type":"markdown","source":["# Testing"],"metadata":{"id":"j-g_UC99Juim"}},{"cell_type":"code","source":["#!pip install transformers\n","\n","from transformers import GPT2LMHeadModel, GPT2Tokenizer\n","import torch\n","\n","# Load the fine-tuned model and tokenizer\n","model_name_or_path = \"./fine_tuned_model\"\n","model = GPT2LMHeadModel.from_pretrained(model_name_or_path)\n","tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)\n","\n","# Set the pad_token to eos_token if it's not already set\n","if tokenizer.pad_token is None:\n","    tokenizer.pad_token = tokenizer.eos_token\n","\n","# Move the model to GPU if available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","\n","# Define the text generation function\n","def generate_text(model, tokenizer, seed_text, max_length=100, temperature=1.0, num_return_sequences=1):\n","    # Tokenize the input text with padding\n","    inputs = tokenizer(seed_text, return_tensors='pt', padding=True, truncation=True)\n","\n","    input_ids = inputs['input_ids'].to(device)\n","    attention_mask = inputs['attention_mask'].to(device)\n","\n","    # Generate text\n","    with torch.no_grad():\n","        output = model.generate(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            max_length=max_length,\n","            temperature=temperature,\n","            num_return_sequences=num_return_sequences,\n","            do_sample=True,\n","            top_k=50,\n","            top_p=0.50,\n","            pad_token_id=tokenizer.eos_token_id  # Ensure padding token is set to eos_token_id\n","        )\n","\n","    # Decode the generated text\n","    generated_texts = []\n","    for i in range(num_return_sequences):\n","        generated_text = tokenizer.decode(output[i], skip_special_tokens=True)\n","        generated_texts.append(generated_text)\n","\n","    return generated_texts\n","\n","# Test the model\n","seed_text = input(\"Enter seed text: \")\n","generated_texts = generate_text(model, tokenizer, seed_text, max_length=50, temperature=0.001, num_return_sequences=1)\n","\n","for i, text in enumerate(generated_texts):\n","    print(f\"Generated Text {i + 1}:\\n{text}\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ulNaiJDXyBDz","executionInfo":{"status":"ok","timestamp":1719478158182,"user_tz":-330,"elapsed":7050,"user":{"displayName":"GOKUL M","userId":"10869017017326738276"}},"outputId":"5fabc648-e414-46da-f846-3c65ab653f73"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Enter seed text: guuvi is\n","Generated Text 1:\n","guuvi is an online platform to learn computer programming based in ind ia it offers free and paid coding courses to students and working professionals in ind ian languages such as hind i, tel ugu, k ann ada, b eng ali\n","\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyNDSlE9hzBMrZ/uK9a33/M1"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}